---
title: "Evaluation des algorithmes de ranking pour classer des produits sur un site de e-commerce"
output: pdf_document
date: "2025-12-08"
author: "Flavie Bertrand et Marion Tremblay" 
header-includes:
  - \usepackage{amsmath}
  - \usepackage{amssymb}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(Ranking)
library(kableExtra)
library(dplyr)
library(ggplot2)
```

# 1. Description du problème et objectif

## 1.1. Problème général de ranking 

On considère :

- \( i \in \{1, 2, \ldots, n\} \) : l'élément i
- un score associé à chaque élément : \( s_i \in \mathbb{R} \)
- une variable binaire \( x_{i,p} \in \{0,1\} \) qui vaut 1 si l'élément \( i \) est placé à la position \( p \) (0 sinon)
- un poids \( w_p \) associé à la position \( p \)

L'objectif est de déterminer un classement des \( k \) premiers éléments maximisant le score total.

\[
\max_x \sum_{i=1}^n \sum_{p=1}^k w_p\, s_i\, x_{i,p}
\]

**Sous contraintes : **

- Un élément ne peut occuper au plus qu'une seule position :

\[
\sum_{p=1}^k x_{i,p} \le 1,\quad \forall i
\]

- Chaque position doit être occupée par exactement un élément :

\[
\sum_{i=1}^m x_{i,p} = 1,\quad \forall p
\]

- Contraintes de groupes :

On définit des groupes :

\[
G_g \subseteq \{1,\ldots,m\}, \quad g = 1, \ldots, G
\]

On impose que chaque groupe \( G_g \) apparaisse au plus \( \alpha_g \) fois dans les \( k \) premiers :

\[
\sum_{i \in G_g} \sum_{p=1}^k x_{i,p} \le \alpha_g,\quad \forall g
\]


## 1.2. Exemple concret 

### 1.2.1. Contexte 

On applique ce problème pour sélectionner les 10 produits les plus pertinents pour les afficher sur la page d'accuei d'un site de e-commerce. Chaque produit a un score $s_i$ correspondant à la moyenne des notes données par les utilisateurs. On souhaite maximiser ce score tout en respectant certaines contraintes :
- Diversité des catégories : pas plus de 3 produits de la même catégorie 
- Limiter la domination d'une marque : pas plus de 2 produits de la même marque 
- Contrainte marketing : au moins un produit sponsorisé doit apparaître dans le top 10

Pour répondre à ce problème on utilise la base de données ...

### 1.2.2. Formulation mathématique 

La fonction objectif est :

\[
\max \sum_{k=1}^{10} w_k \, s_{\pi(k)}
\]

où :

- \(\pi(k)\) est le produit placé à la position \(k\),
- \(w_k\) est le poids associé à la position \(k\).

**Sous contraintes**

- Chaque position contient exactement un produit :

\[
\sum_{i=1}^{n} x_i^k = 1 \quad \forall k
\]

- Un produit ne peut être affiché qu'une seule fois :

\[
\sum_{k=1}^{10} x_i^k \le 1 \quad \forall i
\]

- Maximum 3 produits par catégorie :

\[
\sum_{i \in \text{cat } c} \sum_{k=1}^{10} x_i^k \le 3
\]

- Maximum 2 produits par marque : 

\[
\sum_{i \in \text{brand } c} \sum_{k=1}^{10} x_i^k \le 2
\]

- Au moins 1 produit sponsorisé dans le top 10 :

\[
\sum_{i \in \text{sponsor}} \sum_{k=1}^{10} x_i^k \ge 1
\]

Cette dernière contrainte est équivalente à avoir maximum 9 produits non sponsorisé dans le top 10, soit :
\[
\sum_{i \in \text{no sponsor}} \sum_{k=1}^{10} x_i^k \le 9
\]







# 2. Approche heuristique 

L'approche **gloutonne** constitue une première stratégie simple pour résoudre le problème de ranking pondéré avec contraintes de groupes.


### Principe algorithmique

1. Initialiser les compteurs de chaque groupe à 0 et liste des éléments sélectionnés vide.

2. Pour chaque position (p = 1,\dots,k) :

   * Considérer les éléments non encore sélectionnés respectant les contraintes de groupes.
   * Choisir l'élément avec le gain immédiat maximal $w_p \cdot s_i$
   * Mettre à jour les compteurs de groupes et le score total.

3. Arrêt si aucune sélection possible pour une position ou si toutes les positions sont remplies.

Cette méthode est **très rapide** et facile à implémenter, mais elle ne garantit **pas d'optimalité globale**, car elle ne considère pas les effets futurs de chaque choix.

### Exemple illustratif

```{r, message=FALSE, echo=FALSE}
data <- data.frame(
  id = 1:10,
  score = c(1, 9, 8, 7, 6, 5, 4, 3, 2, 10),
  groupes = c("C", "A", "B", "B", "C", "A,B", "C", "A", "B", "A")
)

max_par_groupe <- list(A = 2, B = 2, C = 1)
poids <- c(5,4,3,2,1)
k <- 5

res_naif <- ranking_naif_max(data, k, max_par_groupe, poids_positions = poids)

res_naif$selected_items
res_naif$best_score
```


* L'algorithme privilégie **les éléments les mieux notés pour les premières positions**.
* Les contraintes de groupe peuvent limiter le choix, ce qui peut conduire à **des choix sous-optimaux pour les positions suivantes**.
* Malgré ses limites, cette approche **rapide et simple** est utile pour obtenir une **baseline** ou comparer avec des méthodes plus sophistiquées comme la programmation dynamique exacte ou le Beam Search.







# 3. Solution 1 : programmation dynamique 

## Idée générale

Pour résoudre le problème de ranking sous contraintes de groupe et de poids positionnels, on utilise une programmation dynamique :

Chaque état est défini par :
- le nombre de positions déjà remplies,
- le nombre d'éléments choisis par groupe (compteurs de groupes).
- À chaque étape (position p), on essaie tous les éléments possibles qui respectent les contraintes de groupes et qui n'ont pas déjà été choisis.

Pour chaque état, on garde le meilleur score pondéré atteint.

À la fin, on récupère l'état final avec le score total maximal et la sélection correspondante.

## Exemple concret

On applique l'algorithme sur un petit jeu de données illustratif :
- 10 produits avec un score utilisateur (score) et un ou plusieurs groupes (groupes).
- Contraintes : max 2 produits du groupe A, 2 du groupe B, 1 du groupe C.
- Poids décroissants pour les positions : 5, 4, 3, 2, 1.


```{r, echo=FALSE, message=FALSE}
# Données d'exemple
data <- data.frame(
  id = 1:10,
  score = c(1, 9, 8, 7, 6, 5, 4, 3, 2, 10),
  groupes = c("C", "A", "B", "B", "C", "A,B", "C", "A", "B", "A")
)

# Contraintes: max 2 éléments du groupe A, 2 du groupe B, 1 du groupe C
max_par_groupe <- list(A = 2, B = 2, C = 1)

# Poids décroissants pour les positions (position 1 = poids 5, position 2 = poids 4, etc.)
poids <- c(5, 4, 3, 2, 1)

# Résolution
result <- ranking_max(data, k = 5, max_par_groupe, poids_positions = poids)

result$selected_items %>%
  select(position, id, score, group_list, poids_position, score_pondere) %>%
  kable(
    format = "latex",              # forcer le format LaTeX
    booktabs = TRUE,               # plus joli pour LaTeX
    align = "c",
    col.names = c("Position", "ID", "Score", "Groupe", "Poids position", "Score pondéré"),
    caption = "Tableau des éléments sélectionnés par l'algorithme ranking max",
    escape = TRUE                  # <- très important pour échapper les caractères spéciaux
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position"),  # bootstrap_options = c(...) est pour HTML, ici LaTeX
    full_width = FALSE
  ) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = TRUE) %>%
  column_spec(6, bold = TRUE)

```

Score total = 130

Visualisation : contribution par position
```{r, out.width='80%', echo=FALSE, message=FALSE}
result$score_pondere <- result$score * result$poids

df <- result$selected_items

ggplot(df, aes(x = factor(position), y = score_pondere)) +
  geom_col(fill="steelblue") +
  labs(title="Contribution pondérée par position",
       x="Position", y="Score pondéré") +
  theme_minimal()
```

Le graphique montre que les positions les mieux pondérées (1 et 2) contribuent le plus au score total. Il permet de visualiser l'impact de chaque choix sur le score global.


# 4. Solution 2 : programmation dynamique et tas 

Pour des ensembles de données plus volumineux, la **programmation dynamique exacte** peut devenir très coûteuse en temps et en mémoire, car le nombre d'états possibles croît de manière exponentielle avec le nombre de positions (k) et le nombre de groupes (G).

Pour pallier cette limitation, nous utilisons une approche **approximate avec Beam Search**, elle conserve uniquement beam_size meilleurs états à chaque niveau, on a alors une approximation proche de l'optimum.

### Principe algorithmique

1. Représentation des états, chaque état est défini par :

   * le score cumulé des éléments sélectionnés,
   * le compteur d'éléments choisis par groupe,
   * une clé unique codant ces compteurs.

2. Trier les éléments par score décroissant.

3. Pour chaque position :

   * Pour chaque état conservé, on génère de nouveaux états en ajoutant chaque élément disponible respectant les contraintes.
   * On ajoute ces états dans un max-heap et ne garder que beam_size meilleurs états (pruning).

3. À la fin, on reconstruit la sélection à partir du meilleur état final.

La largeur du beam (`beam_size`) permet de **contrôler le compromis précision / performance**.


### Un exemple

On reprend l'exemple précédent des 10 produits, avec un **beam de taille 5** :

```{r, warning=FALSE, echo=FALSE, message=FALSE}
library(Rcpp)

data <- data.frame(
  id = 1:10,
  score = c(1, 9, 8, 7, 6, 5, 4, 3, 2, 10),
  groupes = c("C", "A", "B", "B", "C", "A,B", "C", "A", "B", "A")
)

max_par_groupe <- list(A=2, B=2, C=1)
k <- 5

res <- ranking_max_dp_heap_cpp(data, k, max_par_groupe, beam_size = 5)
print(res)
```

* `res$best_score` donne le score cumulé approximatif.
* `res$selected_items` contient la liste des éléments sélectionnés.
* Le champ `approximation` indique si la solution a été tronquée par le beam.

* Même avec un **beam très limité**, l'algorithme tend à **sélectionner les éléments avec les meilleurs scores** tout en respectant les contraintes de groupe.
* La **programmation dynamique combinée à un heap** permet de **garder les meilleures solutions intermédiaires** sans générer tous les états possibles, ce qui est crucial pour de grands ensembles de données.
* Ce compromis entre **exactitude et performance** est particulièrement utile dans les systèmes de recommandation ou e-commerce où le calcul doit rester rapide.











# 5. Comparaison des résultats 



# 6. Complexité des algorithmes (par le calcul)

Voici nos 4 algorithmes :

1. **Algorithme naïf R**
2. **Algorithme dynamique R**
3. **Algorithme dynamique C++ **
4. **Algorithme dynamique C++ amélioré (beam search)**

Pour chacun, nous analysons la complexité **temps** (meilleur, pire, moyenne)
Les paramètres en jeu sont :  
- **n** : nombre d'items (taille de l'entrée)
- **G** : nombre de groupes
- **k** : nombre de positions à remplir
- **S** : nombre d'états effectivement atteints dans la DP
- **M** : nombre théorique maximal d'états = $\prod_{g=1}^G (\text{max\_cap}_g + 1)$
- **$\bar{g}$** : nombre moyen de groupes par item


## 1. Algorithme Naïf Glouton (R)

Sélection gloutonne : pour chaque position p, parcourir tous les candidats restants et choisir celui au meilleur gain immédiat $w_p \times \text{score}_i$.

### Analyse détaillée

**Étape 1 - Initialisation** : $O(G)$

- Création des structures de données
- Négligeable devant la boucle principale

**Étape 2 - Boucle principale**

Pour chaque position $p = 1, \ldots, k$ :

- Nombre d'items restants à examiner : $n - (p-1)$
- Pour chaque item candidat :
  - Parser les groupes : $O(\bar{g})$ 
  - Vérifier les contraintes : $O(\bar{g})$ comparaisons
  - Calculer le gain : $O(1)$
- Mise à jour des compteurs : $O(\bar{g})$

Coût pour la position $p$ :
$$T_p = O((n - p + 1) \times \bar{g})$$

Coût total de la boucle :
$$T_{\text{boucle}} = \sum_{p=1}^{k} O((n - p + 1) \times \bar{g}) = O\left(\bar{g} \times \left(kn - \frac{k(k-1)}{2}\right)\right) \approx O(k n \bar{g})$$

### Complexité temporelle

$$\boxed{T_{\text{naïf}}(n) = O(k n \bar{g})}$$

**Meilleur cas** : $\boxed{T(n) = O(kn)}$ si $\bar{g} = O(1)$

**Cas moyen** : $\boxed{T(n) = O(k n \bar{g})}$

**Pire cas** : $\boxed{T(n) = O(k n G)}$ si $\bar{g} = G$



## 2. Algorithme DP (R)

Programmation dynamique avec états définis par $(p, c_1, \ldots, c_G)$ où $c_g$ = nombre d'items du groupe $g$ déjà utilisés.

### Analyse détaillée

**Étape 1 - Prétraitement** : $O(nG)$

- Parser les groupes de chaque item : $O(n \bar{g})$
- Créer la matrice binaire `item_groups` : $O(nG)$

**Étape 2 - Programmation dynamique**

Structure : `DP[[p+1]][[key]]` avec `key = "c1,c2,...,cG"`

Pour chaque position $p = 1, \ldots, k$ :

- États actifs au niveau $p-1$ : $S_p$ états
- Pour chaque état actif :
  - Parser la clé : $O(G)$
  - Pour chaque item $i = 1, \ldots, n$ :
    - Vérifier si utilisé : $O(p)$ 
    - Calculer nouveaux compteurs : $O(G)$
    - Vérifier contraintes : $O(G)$
    - Créer nouvelle clé : $O(G)$
    - Mise à jour DP : $O(1)$

Coût pour un état et un item : $O(p + G)$

Coût pour la position $p$ : $T_p = O(S_p \times n \times (p + G))$

En supposant $S_p \approx S$ constant et $G \geq k$ :

$$T_{\text{DP}} = \sum_{p=1}^{k} O(S \times n \times G) = O(S n k G)$$

**Étape 3 - Recherche du meilleur** : $O(S \times G)$

### Complexité temporelle

$$\boxed{T_{\text{DP-R}}(n) = O(S n k G)}$$

**Meilleur cas** : $\boxed{T(n) = O(nkG)}$ si $S = O(1)$

**Cas moyen** : $\boxed{T(n) = O(SnkG)}$ avec $S \ll M$

**Pire cas** : $\boxed{T(n) = O(MnkG)}$ où $M = \prod_{g=1}^G (\text{max\_cap}_g + 1)$ (exponentiel en $G$)


## 3. Algorithme DP (C++)

Même logique que DP R, mais avec `unordered_map` et optimisations C++.

### Analyse détaillée

**Prétraitement** : $O(nG)$

- Parsing avec `stringstream` : $O(n\bar{g})$
- Construction matrice : $O(nG)$

**Programmation dynamique**

Pour chaque position $p$, chaque état $S_p$, chaque item $n$ :

- Parse key : $O(G)$
- Vérifier si utilisé (std::find) : $O(p)$
- Calculer compteurs : $O(G)$
- Make key : $O(G)$
- Hash + lookup : $O(G)$ pour le hashing, $O(1)$ amorti pour l'accès
- Insert/update : $O(1)$ amorti

Coût par transition : $O(p + G)$

Si $G \geq k$ : $T_{\text{DP}} = O(SnkG)$

### Complexité temporelle

$$\boxed{T_{\text{DP-C++}}(n) = O(S n k G)}$$

**Meilleur cas** : $\boxed{T(n) = O(nkG)}$ si $S = O(1)$

**Cas moyen** : $\boxed{T(n) = O(SnkG)}$

**Pire cas** : $\boxed{T(n) = O(MnkG)}$ avec $M$ exponentiel en $G$


**Note** : Même complexité asymptotique que DP R, mais facteur constant 5-20× plus petit en pratique.


## 4. Algorithme dynamique amélioré (C++)

DP avec pruning : conserve uniquement les `beam_size` meilleurs états à chaque niveau.

### Analyse détaillée

Soit $B = \min(\text{beam\_size}, M)$ le nombre effectif d'états conservés.

**Étape 1 - Tri initial** : $O(n \log n)$

Items triés par score décroissant pour améliorer la qualité des états gardés.

**Étape 2 - DP avec tas (priority_queue)**

Pour chaque position $p$, chaque état (max $B$), chaque item $n$ :

- Extraction des états du tas : $O(B \log B)$ (une fois par niveau)
- Pour chaque état × item :
  - Calcul compteurs : $O(G)$
  - Make key : $O(G)$
  - Push dans tas : $O(\log B)$
  - Pruning si nécessaire : $O(\log B)$

Coût par niveau : $T_p = O(B \log B) + O(nB(G + \log B))$

Si $G \geq \log B$ (généralement vrai) :

$$T_p = O(nBG)$$

Sur $k$ niveaux :

$$T_{\text{DP}} = O(nkBG)$$

**Étape 3 - Recherche meilleur** : $O(kB)$

**Étape 4 - Reconstruction** : $O(n)$

### Complexité temporelle

$$\boxed{T_{\text{Beam}}(n) = O(n \log n + nkBG)}$$

Si $nkBG \gg n \log n$ : $\boxed{T_{\text{Beam}}(n) = O(nkBG)}$

**Meilleur cas** : $\boxed{T(n) = O(n \log n + nkG)}$ si $B = 1$

**Cas moyen** : $\boxed{T(n) = O(n \log n + nkBG)}$ avec $B$ modéré (100-10000)

**Pire cas** : $\boxed{T(n) = O(n \log n + nkBG)}$ (même formule, $B$ peut être grand)


**Avantage majeur** : Complexité **contrôlable** via `beam_size`, contrairement aux versions exactes.



## Tableau récapitulatif

| Algorithme           | Meilleur cas            | Cas moyen              | Pire cas                    |
|----------------------|-------------------------|------------------------|-----------------------------|
| **Naïf R**           | $O(kn)$                 | $O(kn\bar{g})$         | $O(knG)$                    |
| **DP R**             | $O(nkG)$                | $O(SnkG)$              | $O(MnkG)$                   |
| **DP C++**           | $O(nkG)$                | $O(SnkG)$              | $O(MnkG)$                   |
| **Beam C++**         | $O(n \log n + nkG)$     | $O(n \log n + nkBG)$   | $O(n \log n + nkBG)$        |


## Comparaison graphique

```{r complexity-comparison, echo=FALSE, message=FALSE, warning=FALSE, fig.width=12, fig.height=6}
library(ggplot2)
library(dplyr)
library(tidyr)
library(scales)

# Paramètres de simulation
n_values <- seq(100, 2000, length.out = 200)
G <- 5
k <- 20
g_bar <- 2  # nombre moyen de groupes par item
S <- 200    # États moyens atteints
B <- 1000   # Beam size
M <- (3+1)^G  # Pire cas avec cap_g = 3 pour chaque groupe

# === CAS MOYEN ===
df_moyen <- data.frame(
  n = n_values,
  Naive = k * n_values * g_bar,
  DP_R = S * n_values * k * G,
  DP_Cpp = S * n_values * k * G / 10,  # Facteur constant 10x plus petit
  Beam = n_values * log(n_values) + B * n_values * k * G
)

df_moyen_long <- df_moyen %>%
  pivot_longer(-n, names_to = "algo", values_to = "operations") %>%
  mutate(algo = factor(algo, levels = c("Naive", "DP_R", "DP_Cpp", "Beam")))

p1 <- ggplot(df_moyen_long, aes(x = n, y = operations, color = algo)) +
  geom_line(size = 1.2) +
  scale_y_log10(
    labels = comma,
    breaks = 10^(3:10)
  ) +
  scale_color_manual(
    values = c(
      "Naive" = "#E74C3C",
      "DP_R" = "#3498DB",
      "DP_Cpp" = "#2ECC71",
      "Beam" = "#9B59B6"
    ),
    labels = c(
      "Naive" = "Naïf R",
      "DP_R" = "DP R",
      "DP_Cpp" = "DP C++",
      "Beam" = "Beam C++"
    )
  ) +
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "bottom",
    plot.title = element_text(face = "bold", size = 16),
    panel.grid.minor = element_blank()
  ) +
  labs(
    title = sprintf("Cas moyen (G=%d, k=%d, S=%d, B=%d)", G, k, S, B),
    x = "Nombre d'items (n)",
    y = "Nombre d'opérations (échelle log)",
    color = "Algorithme"
  )

# === PIRE CAS ===
df_pire <- data.frame(
  n = n_values,
  Naive = k * n_values * G,
  DP_R = pmin(M * n_values * k * G, 1e12),  # Limité pour visibilité
  DP_Cpp = pmin(M * n_values * k * G / 10, 1e12),
  Beam = n_values * log(n_values) + B * n_values * k * G
)

df_pire_long <- df_pire %>%
  pivot_longer(-n, names_to = "algo", values_to = "operations") %>%
  mutate(algo = factor(algo, levels = c("Naive", "DP_R", "DP_Cpp", "Beam")))

p2 <- ggplot(df_pire_long, aes(x = n, y = operations, color = algo)) +
  geom_line(size = 1.2) +
  scale_y_log10(
    labels = comma,
    breaks = 10^(3:12)
  ) +
  scale_color_manual(
    values = c(
      "Naive" = "#E74C3C",
      "DP_R" = "#3498DB",
      "DP_Cpp" = "#2ECC71",
      "Beam" = "#9B59B6"
    ),
    labels = c(
      "Naive" = "Naïf R",
      "DP_R" = "DP R (plafonné)",
      "DP_Cpp" = "DP C++ (plafonné)",
      "Beam" = "Beam C++"
    )
  ) +
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "bottom",
    plot.title = element_text(face = "bold", size = 16),
    panel.grid.minor = element_blank()
  ) +
  labs(
    title = sprintf("Pire cas (G=%d, k=%d, M=%s, B=%d)", G, k, comma(M), B),
    x = "Nombre d'items (n)",
    y = "Nombre d'opérations (échelle log)",
    color = "Algorithme",
    caption = "Note: DP R et DP C++ plafonnés à 10^12 pour la lisibilité"
  )

# Afficher les graphiques
print(p1)
print(p2)
```

### Observations

**Cas moyen** :

- Beam Search et DP C++ sont très compétitifs
- DP R significativement plus lent (facteur 10)
- Naïf acceptable pour petites instances

**Pire cas** :

- Explosion exponentielle des DP exact (R et C++)
- Beam Search reste linéaire et prévisible
- Seul algorithme viable pour grands problèmes avec $G \geq 6$


## Impact du nombre de groupes G

```{r impact-groups, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=6}
library(ggplot2)
library(dplyr)
library(tidyr)
library(scales)

# Paramètres fixes
n <- 1000
k <- 20
S <- 200
B <- 1000
cap <- 3  # capacité moyenne par groupe

# Variation de G
G_values <- 2:10

# Calcul de M pour chaque G
M_values <- (cap + 1)^G_values

df_impact <- data.frame(
  G = G_values,
  M = M_values,
  Naive = k * n * G_values,
  DP_moyen = S * n * k * G_values,
  DP_pire = M_values * n * k * G_values,
  Beam = B * n * k * G_values
)

# On limite DP_pire pour la visualisation
df_impact$DP_pire_display <- pmin(df_impact$DP_pire, 1e15)

df_impact_long <- df_impact %>%
  select(G, Naive, DP_moyen, DP_pire_display, Beam) %>%
  pivot_longer(-G, names_to = "algo", values_to = "operations") %>%
  mutate(
    algo = factor(
      algo,
      levels = c("Naive", "DP_moyen", "DP_pire_display", "Beam"),
      labels = c("Naïf", "DP cas moyen", "DP pire cas", "Beam")
    )
  )

ggplot(df_impact_long, aes(x = G, y = operations, color = algo)) +
  geom_line(size = 1.2) +
  geom_point(size = 3) +
  scale_y_log10(
    labels = comma,
    breaks = 10^seq(4, 16, by = 2)
  ) +
  scale_x_continuous(breaks = G_values) +
  scale_color_manual(
    values = c(
      "Naïf" = "#E74C3C",
      "DP cas moyen" = "#2ECC71",
      "DP pire cas" = "#3498DB",
      "Beam" = "#9B59B6"
    )
  ) +
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "bottom",
    plot.title = element_text(face = "bold", size = 16),
    panel.grid.minor = element_blank()
  ) +
  labs(
    title = sprintf("Impact du nombre de groupes G (n=%d, k=%d)", n, k),
    x = "Nombre de groupes (G)",
    y = "Nombre d'opérations (échelle log)",
    color = "Algorithme",
    caption = sprintf("Capacité moyenne par groupe = %d | DP pire cas plafonné à 10^15", cap)
  )
```

### Analyse

- **G $\le$ 4** : Tous les algorithmes sont viables
- **G = 5-6** : DP commence à être problématique en pire cas
- **G >= 7** : Seul Beam Search reste praticable
- L'explosion de M = (cap+1)^G rend DP exact inutilisable pour grand G

**Choix de l'algorithme selon le contexte**

| Contexte | Algorithme recommandé | Justification |
|----------|----------------------|---------------|
| Petits problèmes (n<100, G<3, k<10) | **Naïf R** ou **DP R** | Simple, rapide, optimal |
| Moyens problèmes (n<1000, G<5, k<20) | **DP C++** | Optimal garanti, performance acceptable |
| Grands problèmes (n>1000 ou G>5) | **Beam C++** | Seul viable, quasi-optimal |
| Production avec contraintes temps | **Beam C++** (B=5000) | Prédictible, contrôlable |
| Recherche d'optimum prouvé | **DP C++** si faisable | Exact mais peut échouer |


$$
\text{Si } M = \prod_{g=1}^G (\text{max\_cap}_g + 1) > 100000 \Rightarrow \text{Utiliser Beam Search}
$$

# 7. Temps de calcul 

```{r}

n <- c(20,22,25,32,42,55,70,90,125,160)
temps_moyens1 <- numeric(length(n))
temps_moyens2 <- numeric(length(n))
temps_moyens3 <- numeric(length(n))
temps_moyens4 <- numeric(length(n))
k <- 5

df <- read.csv("amazon_products_250.csv", stringsAsFactors = FALSE)
df$sponsored <- ifelse(df$sponsored == "True", "Sponsored", "Not_sponsored")

# Poids de position 
poids_positions <- seq(k, 1, by = -1) 

# Création des listes de contraintes
liste_cat <- setNames(as.list(rep(2, length(unique(df$category)))),
                      unique(df$category))
max_par_groupe <- c(liste_cat, list(Not_sponsored = 9))

for (i in 1:length(n))
{
  echantillon <- df[sample(nrow(df), min(n[i], nrow(df))), ]
  data_mat <- data.frame(
    id = 1:nrow(echantillon),
    score = echantillon$score,
    groupes = paste(echantillon$category, 
                    echantillon$sponsored, sep = ",")
  )
  
  start_time1 <- Sys.time()
  res1 <- ranking_naif_max(data_mat, k = k, max_par_groupe, poids_positions = poids_positions)
  end_time1 <- Sys.time()
  elapsed_time1 <- as.numeric(difftime(end_time1, start_time1, units = "secs"))
  temps_moyens1[i] <- elapsed_time1
  
  start_time2 <- Sys.time()
  res2 <- ranking_max(data_mat, k = k, max_par_groupe, poids_positions = poids_positions)
  end_time2 <- Sys.time()
  elapsed_time2 <- as.numeric(difftime(end_time2, start_time2, units = "secs"))
  temps_moyens2[i] <- elapsed_time2
  
  start_time3 <- Sys.time()
  res3 <- ranking_max_cpp(data_mat, k = k, max_par_groupe, poids_positions = poids_positions)
  end_time3 <- Sys.time()
  elapsed_time3 <- as.numeric(difftime(end_time3, start_time3, units = "secs"))
  temps_moyens3[i] <- elapsed_time3
  
  start_time4 <- Sys.time()
  res4 <- ranking_max_dp_heap_cpp(data_mat, k = k, max_par_groupe, poids_positions = poids_positions)
  end_time4 <- Sys.time()
  elapsed_time4 <- as.numeric(difftime(end_time4, start_time4, units = "secs"))
  temps_moyens4[i] <- elapsed_time4
}

cat("Temps pour l'algorithme naif\n")
temps_moyens1
cat("Temps pour l'algorithme de programmation dynamique\n")
temps_moyens2
cat("Temps pour l'algorithme de programmation dynamique (en C++)\n")
temps_moyens3
cat("Temps pour l'algorithme de programmation dynamique avec tas (en C++)\n")
temps_moyens4

```



```{r}
par(mfrow=c(2,2))

# Graphique 1
res1 <- data.frame(
  n = n,
  time = temps_moyens1
)
plot(log(res1$n), log(res1$time), pch=16,
     xlab="Taille des séquences : n (log)",
     ylab="Temps (s, log)",
     main="Algo naif (log-log)")
abline(lm(log(time) ~ log(n), data=res1), col="red")

# Graphique 2
res2 <- data.frame(
  n = n,
  time = temps_moyens2
)
plot(log(res2$n), log(res2$time), pch=16,
     xlab="Taille des séquences : n (log)",
     ylab="Temps moyen (s, log)",
     main="Algo dp (R) (log-log)")
abline(lm(log(time) ~ log(n), data=res2), col="red")

# Graphique 3
res3 <- data.frame(
  n = n,
  time = temps_moyens3
)
plot(log(res3$n), log(res3$time), pch=16,
     xlab="Taille des séquences : n (log)",
     ylab="Temps moyen (s, log)",
     main="Algo dp (C++) (log-log)")
abline(lm(log(time) ~ log(n), data=res3), col="red")

# Graphique 4
res4 <- data.frame(
  n = n,
  time = temps_moyens4
)
plot(log(res4$n), log(res4$time), pch=16,
     xlab="Taille des séquences : n (log)",
     ylab="Temps moyen (s, log)",
     main="Algo dp avec tas (C++) (log-log)")
abline(lm(log(time) ~ log(n), data=res4), col="red")

par(mfrow=c(1,1))
```

```{r}

modele1 <- lm(log(time) ~ log(n), data = res1)
summary(modele1)

modele2 <- lm(log(time) ~ log(n), data = res2)
summary(modele2)

modele3 <- lm(log(time) ~ log(n), data = res3)
summary(modele3)

modele4 <- lm(log(time) ~ log(n), data = res4)
summary(modele4)
```


```{r}
# Couleurs pour chaque algorithme
cols <- c("blue", "red", "green", "purple")

# Déterminer les limites de l'axe x et y pour inclure toutes les données
xlim <- range(log(res1$n), log(res2$n), log(res3$n), log(res4$n))
ylim <- range(log(res1$time), log(res2$time), log(res3$time), log(res4$time))

# Graphique vide pour préparer l'affichage
plot(NA, NA, xlim=xlim, ylim=ylim,
     xlab="Taille des séquences : n (log)",
     ylab="Temps (s, log)",
     main="Comparaison des algorithmes (log-log)")

# Ajouter les 4 séries de points
points(log(res1$n), log(res1$time), pch=16, col=cols[1])
points(log(res2$n), log(res2$time), pch=16, col=cols[2])
points(log(res3$n), log(res3$time), pch=16, col=cols[3])
points(log(res4$n), log(res4$time), pch=16, col=cols[4])

# Ajouter les droites de régression
abline(lm(log(time) ~ log(n), data=res1), col=cols[1], lwd=2)
abline(lm(log(time) ~ log(n), data=res2), col=cols[2], lwd=2)
abline(lm(log(time) ~ log(n), data=res3), col=cols[3], lwd=2)
abline(lm(log(time) ~ log(n), data=res4), col=cols[4], lwd=2)

# Ajouter la légende
legend("topleft",
       legend=c("Algo naif", "DP (R)", "DP (C++)", "DP + tas (C++)"),
       col=cols, pch=16, lwd=2,
       cex=0.6)

```



